# Copy this file to .env and add your credentials

# Anthropic API (Required)
ANTHROPIC_API_KEY=your-api-key-here

# Confluence Integration (Optional)
# Get API token: https://id.atlassian.com/manage-profile/security/api-tokens
CONFLUENCE_BASE_URL=https://your-domain.atlassian.net
CONFLUENCE_USERNAME=your-email@example.com
CONFLUENCE_API_TOKEN=your-confluence-api-token
CONFLUENCE_DEFAULT_SPACE=your-space-key

# Jira Integration (Optional)
# Get API token: https://id.atlassian.com/manage-profile/security/api-tokens
JIRA_BASE_URL=https://your-domain.atlassian.net
JIRA_USERNAME=your-email@example.com
JIRA_API_TOKEN=your-jira-api-token
JIRA_DEFAULT_PROJECT=your-project-key

# =============================================================================
# LLM Provider Configuration
# =============================================================================
# Select which provider to use: anthropic, openai, gemini, ollama
LLM_PROVIDER=anthropic

# OpenAI Configuration (Optional)
# Get API key: https://platform.openai.com/api-keys
OPENAI_API_KEY=your-openai-api-key-here

# Default OpenAI model to use
# Options: gpt-4o, gpt-4o-mini, gpt-4-turbo, gpt-4, gpt-3.5-turbo
OPENAI_MODEL=gpt-4o

# OpenAI organization ID (optional)
OPENAI_ORG_ID=

# OpenAI request timeout in seconds
OPENAI_TIMEOUT=120

# Google Gemini Configuration (Optional)
# Get API key: https://aistudio.google.com/app/apikey
GEMINI_API_KEY=your-gemini-api-key-here

# Default Gemini model to use
# Options: gemini-2.0-flash-exp, gemini-1.5-pro, gemini-1.5-flash, gemini-1.0-pro
GEMINI_MODEL=gemini-2.0-flash-exp

# Gemini request timeout in seconds
GEMINI_TIMEOUT=120

# Ollama Configuration (Optional - for local LLM support)
# Ollama server URL (default: http://localhost:11434)
OLLAMA_BASE_URL=http://localhost:11434

# Default Ollama model to use
# Popular options: llama3.2, llama3.1, codellama, mistral, mixtral, phi3, qwen2.5
OLLAMA_MODEL=llama3.2

# Alternative models for specific tasks (optional)
OLLAMA_CODE_MODEL=codellama
OLLAMA_EMBEDDING_MODEL=nomic-embed-text

# Ollama request timeout in seconds
OLLAMA_TIMEOUT=120
